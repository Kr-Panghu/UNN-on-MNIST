## 

---

## $\textbf{Introduction}$:

There are several challenges for interpretability of (deep) neural networks nowadays: 1. How to understand NN theoretically. 2. How to understand NN functionality. 3. Where to find interpretability of results. Especially when neural networks have high computational complexity, i.e., have large depth, the interpretability of DNN is hard to explain.

In traditional researches for interpretability of DNN, researchers usually try to understand how neural networks underlie the decision making process by visualizing the semantics of interneurons or by inferring the importance scores of the input/interneuron. However, this traditional direction can only understand what neural networks model on the surface, but can not explain and analyze the more essential expressive ability of neural networks. As a result, most of the current interpretability studies cannot be used in the design and training of feedback-guided neural networks.

In this paper, I took a different approach. I investigated the importance of $\textbf{individual neurons}$ at different levels to the prediction accuracy of the entire neural network using $\textbf{three information-theoretic metrics}$:

1. Entropy
2. Mutual Information
3. Kullback-Leibler Selectivity (which will be defined later)

To value the importance of a single neuron, it is obvious that a single point operation of the neural network is required, and the $\textbf{cumulative ablation}$ method is used in this experiment (i.e. $\textbf{node pruning}$), the main steps of cumulative ablation:

* Removing one or more neurons or a layer from the network. The removal can be done by setting the weights or activations of the selected neurons or layer to zero or by excluding them from the network architecture.
* Evaluate the performance of the ablated network on the same task or dataset used for training. This evaluation can involve measuring metrics such as accuracy, loss, or any other relevant performance measure.



## $\textbf{Information theory metrics}$:

**Experiment Basics.** We consider classifification via fully-connected feed-forward NNs. $\mathcal{C}$ denotes classification set, where $|\mathcal{C}| = C$. Dataset $\mathcal{D} = \{(x_1,y_1),\dots,(x_N,y_N) \}$. $x_i$ denotes the $i$-th input and $y_i$ denotes $i$-th output. Denote $h_j^{(i)}(x_l)$ to be the output of the $j$-th neuron in $i$-th hidden layer if given the input $x_l$. $b$ denotes the bias vector. $\sigma:\mathbb{R}\rightarrow \mathbb{R}$ denotes a non-linear activation function (usually ReLU), then we have
$$
\begin{equation}
h_j^{(i)}(x_l) = \sigma(\sum_pw_{p,j}^{(i-1)}h_p^{(i-1)}(x_l) + b_j^{(i)})
\end{equation}
$$


Let $\mathcal{Q}:\mathbb{R}\rightarrow \mathcal{H}$ be a quantizer that maps outputs to a finite set $\mathcal{H}$. Let $Y$ be a random variable over the set $\mathcal{C}$ of classes and $H_j^{(i)}$ a random variable over $\mathcal{H}$, i.e., the output of the $j$-th neuron of the $i$-th hidden layer.

Define the joint distribution of $Y$ and $H_j^{(i)}$ via the joint frequencies of $\{(y_l,Q(h_j^{(i)}(x_l))) \}$ in the validation set. $\forall c \in \mathcal{C}, h \in \mathcal{H}$,
$$
\begin{equation}
P_{Y,H_j^{(i)}}(c,h) = \frac{\sum_{l=1}^N\mathbb{1}[y_l=c,Q(h_j^{(i)}(x_l))=h]}{N}
\end{equation}
$$
where $\mathbb{1}[\cdot]$ is the indicator function.

**Entropy.** It's a matric that quantifies the uncertainty. Denote $\mathbb{H}[\cdot]$ as entropy, then
$$
\begin{equation}
\mathbb{H}(H_j^{(i)}) = -\sum_{h\in\mathcal{H}}P_{H_j^{(i)}}(h)\log P_{H_j^{(i)}}(h)
\end{equation}
$$
**Mutual Information.** The zero entropy of neuron output indicates that it has little relationship with classification performance, but the reverse cannot be true, and the high entropy of neuron output cannot indicate its importance in classification problem, so we introduce the mutual information between neuron output and classification result as a measure of matric. It measures how the knowledge of $H_j^{(i)}$ helps predicting $Y$. Its mathematical form is 
$$
\begin{equation}
I(H_j^{(i)};Y) = \mathbb{H}(H_j^{(i)}) - \mathbb{H}(H_j^{(i)}|Y)
\end{equation}
$$
**Kullback-Leibler Selectivity.** For deeper neurons, its output may play a decisive role in the classification result. Mathematically, for such a neuron there exists a class $y$ s.t. conditional distribution $P_{H_j^{(i)}|Y=y}$ differs significantly from the marginal distribution $P_{H_j^{(i)}}$. Namely, $D_{KL}(P_{H_j^{(i)}|Y=y}\| P_{H_j^{(i)}})$ is large (other researchers call it specifific information). We denote KL selectivity as the maximum specific information over all classes for a measure of neuron importance, as
$$
\begin{equation}
\max_{y\in\mathcal{C}} D_{KL}(P_{H_j^{(i)}|Y=y}\| P_{H_j^{(i)}})
\end{equation}
$$
By definition, KL selectivity is high when the influence of a single neuron on the classification result of a particular class is large.



---

## $\textbf{Experiment Setup}$

We use a trained NN with 2 or 3 hidden layers with 200 neurons each. (totally 400 or 600 neurons). And apply one-bit quatization, i.e., $|T| = 2$, sigmoid threshold = $0.5$. For the output of the activation function, values less than the threshold are mapped to 0, and values greater than or equal to the threshold are mapped to 1. For dataset, we use MNIST, a set of $28*28$ gray-scale images. The dataset is divided into 50000 training samples, 10000 validation samples and 10000 testing samples. For loss function, we apply cross-entropy loss and $L_2$-norm regularization. If bias balancing is applied, its mathematical form is: $w_{j,k}^{(i)}\sum_l\frac{h_j^{(i)}(x_l)}{N} + b_k^{(i+1)}$.



## $\textbf{Results}$

我们发现对整个NN进行node pruning时，Entropy LVF 和 MI LVF 的曲线下降幅度大，相应的推断：可能浅层神经网络的神经元的entropy和MI相对深层的要低，而KL-selectivity相对深层要高。且我发现，进行random pruning时的效果并不差，说明整个神经网络存在比较大的redundancy.

I find that when node pruning is performed on the whole NN, the curves of Entropy LVF and MI LVF decrease greatly, and the corresponding inference is that entropy and MI of neurons in shallow layers are lower than those in deep layers, while KL-selectivity is higher than that in deep layers. And I find that the effect of random pruning is pretty good, indicating that the whole neural network has a relatively large redundancy.

Since shallow neurons receive raw input data or less processed data, it may be easier for them to extract some of the salient features in the data, and thus reduce some of the uncertainty. As a result, shallow neurons may have relatively low entropy.



显然不论是哪一层的pruning，random pruning都是一个适中的选择。除此以外，通过Layer-wise ablation, 还可以总结以下几点insights：

考虑Mutual Information和KL-Selectivity。对于浅层，LVF比HVF是一个更好的选择，这从直觉上解释的通，因为high KL-Selecitivity和high Mutual Information意味着神经元与（某类）分类结果高度相关，因此HVF是不推荐的。但是2-layer和3-layer model都出现了一个现象：when performing pruning for the last layer, KL-Selectivity HVF和MI HVF会明显好于LVF，这有点反直觉。对于这个现象，我会在Conclusion and Conjectures Section中作出一些尝试的解释。

还有一个奇怪的现象，我们直觉上认为对更深层的神经元进行pruning时，效果应当是好于浅层的，这也有一点反直觉，我将会在后面的section作出解释。

Obviously, random pruning is a moderate choice regardless of the level of pruning. In addition, the following insights can be summarized by layer-wise ablation:

Consider Mutual Information and KL-Selectivity. For shallow layers, LVF is a better choice than HVF, which intuitively makes sense because high KL-Selecitivity and high Mutual Information mean that neurons are highly correlated with classification results, and therefore HVF is not recommended. But a phenomenon appears for both 2-layer and 3-layer models: when performing pruning for the last layer, KL-Selectivity HVF and MI HVF will be significantly better than LVF, which is somewhat \textbf{counterintuitive}. I will try to explain this phenomenon in the Conclusion and Conjectures Section.

Another oddness is that our intuition is that pruning deeper neurons should be better than pruning shallower.This is also \textbf{counterintuitive}, as I'll explain in the next section.

---

\paragraph{Explanations of the counterintuitive phenomenon presented in the previous section.}When performing pruning for the last layer, KL-Selectivity HVF and MI HVF are surprisingly better than LVF, which is counterintuitive to some extent.

可能的解释是：1. 过拟合问题。高KL-Selectivity的神经元往往在训练数据中对某个特定类别的分类结果起着重要作用，但这并不意味着它们对于泛化能力更好。在某些情况下，这些高KL-Selectivity的神经元可能过于关注训练数据中的噪声或冗余特征，导致网络过拟合。2. 重要特征的共享。在深度神经网络中，随着网络的向前传播，特征的抽象级别逐渐增加。在最后一层中，神经元可能更多地关注于对整体分类结果的决策贡献，而不是对单个类别的影响。高KL-Selectivity的神经元可能在某个特定类别的分类上有很大影响，但对于整体分类结果来说，其他神经元可能已经学习到了相似的重要特征。

即，对KL-Selectivity和MI的HVF/LVF时结果的反常。

i.e., the unnatural results of HVF/LVF for KL-Selectivity and MI.

这篇论文主要参考了论文Understanding Neural Networks and Individual Neuron Importance via
Information-Ordered Cumulative Ablation，the first of my reference list。在此论文的基础上，我针对性地进行了对于MNIST数据集的拓展实验。

在这论文中，我们采用信息论方法，使用information theoretic metrics for node pruning的方法，即观察删除单个神经元对于整个神经网络性能的影响来研究不同层级的单个神经元在整个神经网络中的重要性，使用Entropy, Mutual information, 以及创新性定义的KL-Selectivity来作为决定神经元删除顺序的指标。同时，我延伸了原论文中采用的2-layer model至3-layer model for more insights。当隐藏层的层数增加时，更能得出generalized、reliable的结论：1. The distribution of the proposed metrics changes from layer to layer. 2. Information theory metrics based cumulative neuron ablation with sorting. 3. Deeper layers may have larger redundancy. 4. 以mutual information和KL-Selectivity作为node pruning的指标具有一定的合理性，说明它们和分类结果相关性较强，但不严格随着层数的增加而增加，一些DNN中的过拟合现象或重要特征的共享可能导致删除较深层的神经元对分类结果的影响小。

用信息论指标研究不同层级上单个神经元在DNN中的重要性



## $\textbf{Conclusion}$

$\textbf{The distribution of the proposed metrics changes from layer to layer}$. Generally, deeper layers have larger values, especially mutual information and KL-Selectivity. Therefore, it's not reasonable for us to compare neurons at different levels for these metrics.

$\textbf{Information theory metrics based cumulative neuron ablation with sorting}$ allows us to formulate hypotheses about the interactions of neurons throughout the layer.

$\textbf{Deeper layers may have larger redundancy}$. As we can see, although the unexpected result of "ablation of neurons in the second layer" was the worst in the 3-layer experiment, in general, the effect of ablation of neurons in the last layer on the performance of the neural network in both the 3-layer experiment and the 2-layer experiment was small. And are much smaller than the previous layers, which shows that deeper layers may have larger redundancy, but this redundancy does not strictly increase with the increase of depth.

The correlation between the considered quantities and neuron importance depends on the depth and structure of the considered layer, and on the NN architecture as a whole.



我们发现使用bias balancing时，对模型的影响整体会减少，且accuracy曲线会变得平稳，这也有助于减小模型的不同训练结果带来的误差（即减小结果的偶然性）。

I find that with bias balancing, the overall impact on the model is reduced and the accuracy curve flattens, which also helps to reduce the error caused by different training results of the model (i.e. reduce the occasionality of the results).